{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853720bd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac3cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cindyzastudil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cindyzastudil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cindyzastudil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d4398",
   "metadata": {},
   "source": [
    "## Read in the CMU Book Summary Dataset\n",
    "\n",
    "The CMU Book Summary Dataset is a collection of 16,559 different book summaries extracted from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48849602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       title  \\\n",
      "0                                Animal Farm   \n",
      "1                         A Clockwork Orange   \n",
      "2                                 The Plague   \n",
      "3  An Enquiry Concerning Human Understanding   \n",
      "4                       A Fire Upon the Deep   \n",
      "\n",
      "                                               genre  \\\n",
      "0  {\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...   \n",
      "1  {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n",
      "2  {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n",
      "3                                                NaN   \n",
      "4  {\"/m/03lrw\": \"Hard science fiction\", \"/m/06n90...   \n",
      "\n",
      "                                             summary  \n",
      "0   Old Major, the old boar on the Manor Farm, ca...  \n",
      "1   Alex, a teenager living in near-future Englan...  \n",
      "2   The text of The Plague is divided into five p...  \n",
      "3   The argument of the Enquiry proceeds by a ser...  \n",
      "4   The novel posits that space around the Milky ...  \n"
     ]
    }
   ],
   "source": [
    "# Read in the data and add columns to the dataframe\n",
    "book_summs = pd.read_csv('data/booksummaries.txt', header=None, sep='\\t')\n",
    "book_summs.columns =['wikipedia_article_id', 'freebase_id', 'title', 'author', 'pub_date', 'genre', 'summary']\n",
    "\n",
    "# Remove extraneous features - wikipedia_article_id, freebase_id, author, pub_date\n",
    "book_summs = book_summs.drop(labels=['wikipedia_article_id', 'freebase_id', 'author', 'pub_date'], axis=1)\n",
    "print(book_summs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e6030",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a37027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code taken from fast-bert\n",
    "def spec_add_spaces(t: str) -> str:\n",
    "    \"Add spaces around / and # in `t`. \\n\"\n",
    "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
    "\n",
    "def rm_useless_spaces(t: str) -> str:\n",
    "    \"Remove multiple spaces in `t`.\"\n",
    "    return re.sub(\" {2,}\", \" \", t)\n",
    "\n",
    "def replace_multi_newline(t: str) -> str:\n",
    "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
    "\n",
    "def clean_text(input_text):\n",
    "    text = replace_multi_newline(input_text)\n",
    "    text = spec_add_spaces(text)\n",
    "    text = rm_useless_spaces(text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f4dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset before preprocessing: 16559\n",
      "Size of dataset after removing missing genres: 12841\n",
      "Size of dataset after removing missing titles: 12841\n",
      "Size of dataset after removing missing summaries: 12841\n",
      "Size of dataset after preprocessing: 12841\n",
      "                            title  \\\n",
      "0                     Animal Farm   \n",
      "1              A Clockwork Orange   \n",
      "2                      The Plague   \n",
      "4            A Fire Upon the Deep   \n",
      "5  All Quiet on the Western Front   \n",
      "\n",
      "                                               genre  \\\n",
      "0  {\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...   \n",
      "1  {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n",
      "2  {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n",
      "4  {\"/m/03lrw\": \"Hard science fiction\", \"/m/06n90...   \n",
      "5  {\"/m/098tmk\": \"War novel\", \"/m/016lj8\": \"Roman...   \n",
      "\n",
      "                                             summary          tokenized_title  \\\n",
      "0   Old Major, the old boar on the Manor Farm, ca...           [animal, farm]   \n",
      "1   Alex, a teenager living in near-future Englan...      [clockwork, orange]   \n",
      "2   The text of The Plague is divided into five p...                 [plague]   \n",
      "4   The novel posits that space around the Milky ...       [fire, upon, deep]   \n",
      "5   The book tells the story of Paul Bäumer, a Ge...  [quiet, western, front]   \n",
      "\n",
      "                                   tokenized_summary  \n",
      "0  [old, major, ,, old, boar, manor, farm, ,, cal...  \n",
      "1  [alex, ,, teenager, living, near-future, engla...  \n",
      "2  [text, plague, divided, five, parts, ., town, ...  \n",
      "4  [novel, posits, space, around, milky, way, div...  \n",
      "5  [book, tells, story, paul, bäumer, ,, german, ...  \n"
     ]
    }
   ],
   "source": [
    "print('Size of dataset before preprocessing:', len(book_summs))\n",
    "\n",
    "# Remove any books which don't have genres\n",
    "book_summs.dropna(subset=['genre'], inplace=True)\n",
    "\n",
    "print('Size of dataset after removing missing genres:', len(book_summs))\n",
    "\n",
    "# Remove any books which don't have titles\n",
    "book_summs.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "print('Size of dataset after removing missing titles:', len(book_summs))\n",
    "\n",
    "# Remove any books which don't have summaries\n",
    "book_summs.dropna(subset=['summary'], inplace=True)\n",
    "\n",
    "print('Size of dataset after removing missing summaries:', len(book_summs))\n",
    "print('Size of dataset after preprocessing:', len(book_summs))\n",
    "\n",
    "# Tokenize titles & convert to lower case - add to tokenized_title column\n",
    "book_summs['tokenized_title'] = book_summs['title'].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))\n",
    "\n",
    "# Tokenize summaries & convert to lower case - add to tokenized_summary column\n",
    "book_summs['tokenized_summary'] = book_summs['summary'].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))\n",
    "\n",
    "# Remove all stop words from all summaries & titles\n",
    "stop = stopwords.words('english')\n",
    "book_summs['tokenized_summary'] = book_summs['tokenized_summary'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "book_summs['tokenized_title'] = book_summs['tokenized_title'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "    \n",
    "print(book_summs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6029cdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            title  \\\n",
      "0                     Animal Farm   \n",
      "1              A Clockwork Orange   \n",
      "2                      The Plague   \n",
      "4            A Fire Upon the Deep   \n",
      "5  All Quiet on the Western Front   \n",
      "\n",
      "                                               genre  \\\n",
      "0  {\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...   \n",
      "1  {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n",
      "2  {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n",
      "4  {\"/m/03lrw\": \"Hard science fiction\", \"/m/06n90...   \n",
      "5  {\"/m/098tmk\": \"War novel\", \"/m/016lj8\": \"Roman...   \n",
      "\n",
      "                                             summary          tokenized_title  \\\n",
      "0   Old Major, the old boar on the Manor Farm, ca...           [animal, farm]   \n",
      "1   Alex, a teenager living in near-future Englan...      [clockwork, orange]   \n",
      "2   The text of The Plague is divided into five p...                 [plague]   \n",
      "4   The novel posits that space around the Milky ...       [fire, upon, deep]   \n",
      "5   The book tells the story of Paul Bäumer, a Ge...  [quiet, western, front]   \n",
      "\n",
      "                                   tokenized_summary  \\\n",
      "0  [old, major, ,, old, boar, manor, farm, ,, cal...   \n",
      "1  [alex, ,, teenager, living, near-future, engla...   \n",
      "2  [text, plague, divided, five, parts, ., town, ...   \n",
      "4  [novel, posits, space, around, milky, way, div...   \n",
      "5  [book, tells, story, paul, bäumer, ,, german, ...   \n",
      "\n",
      "                                     formatted_genre  \n",
      "0  [Roman à clef, Satire, Children's literature, ...  \n",
      "1  [Science Fiction, Novella, Speculative fiction...  \n",
      "2  [Existentialism, Fiction, Absurdist fiction, N...  \n",
      "4  [Hard science fiction, Science Fiction, Specul...  \n",
      "5                          [War novel, Roman à clef]  \n"
     ]
    }
   ],
   "source": [
    "# Format the genre field\n",
    "formatted_genres = []\n",
    "genre_dict = dict()\n",
    "for g in book_summs['genre']:\n",
    "    subg = []\n",
    "    genre_dict = eval(g)\n",
    "    for k in genre_dict.keys():\n",
    "        subg.append(genre_dict[k])\n",
    "    formatted_genres.append(subg)\n",
    "book_summs['formatted_genre'] = formatted_genres\n",
    "print(book_summs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323441e",
   "metadata": {},
   "source": [
    "## Extract data to use for training & testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bada585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from midterm source code (PP1)\n",
    "def generate_vocab_map(df, cutoff=1):\n",
    "    vocab = {\"\": 0, \"UNK\": 1}\n",
    "    reversed_vocab = None\n",
    "    \n",
    "    # Iterate over the tokenized words in the dataset & construct a frequency map\n",
    "    vocab_count = dict()\n",
    "    uid = 2\n",
    "    for i in df['tokenized_title'].tolist():\n",
    "      # Iterate over the list of tokenized words from each summary\n",
    "      for t in i:\n",
    "        if t in vocab_count:\n",
    "            vocab_count[t] += 1\n",
    "        else:\n",
    "            vocab_count[t] = 1\n",
    "    for i in df['tokenized_summary'].tolist():\n",
    "        # Iterate over the list of tokensize words from each title\n",
    "        for t in i:\n",
    "            if t in vocab_count:\n",
    "                vocab_count[t] += 1\n",
    "            else:\n",
    "                vocab_count[t] = 1\n",
    "\n",
    "    # Ignore all words under the cutoff, give all others a unique id\n",
    "    for i in vocab_count.keys():\n",
    "        if vocab_count[i] > cutoff:\n",
    "            vocab[i] = uid\n",
    "            uid += 1\n",
    "    \n",
    "    reversed_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    return vocab, reversed_vocab\n",
    "\n",
    "def generate_genre_map(genres):\n",
    "    genre_map = dict()\n",
    "    uid = 0\n",
    "    for g in genres:\n",
    "        genre_map[g] = uid\n",
    "        uid += 1\n",
    "    return genre_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc035ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique genres: 227\n",
      "{'Computer Science', 'Police procedural', 'Historical fiction', 'Field guide', 'Literary realism', 'Paranormal romance', 'Comedy', 'Historical novel', 'History', 'Treatise', 'Nature', 'Social sciences', 'Zombie', 'Science', 'Essay', 'Psychological novel', 'Education', 'Post-holocaust', 'Comic book', 'Invasion literature', 'Comic novel', 'Dark fantasy', 'Robinsonade', 'Photography', 'Mystery', 'Apocalyptic and post-apocalyptic fiction', 'Black comedy', 'Polemic', 'Epistolary novel', 'High fantasy', 'School story', 'Music', 'Drama', 'Inspirational', 'Youth', 'Heroic fantasy', 'Philosophy', 'Military science fiction', 'Ergodic literature', 'Campus novel', 'Feminist science fiction', 'First-person narrative', 'Spirituality', 'Sociology', 'Anti-nuclear', 'Hardboiled', 'Political philosophy', 'Scientific romance', 'Anthropology', 'Medieval romance', 'Creative nonfiction', 'New York Times Best Seller list', 'Cyberpunk', 'Role-playing game', 'Parallel novel', 'Historical romance', 'Thriller', 'Short story', 'Comic science fiction', 'Conspiracy', 'Prose', 'Tragicomedy', 'Chivalric romance', 'Coming of age', 'War novel', 'Erotica', 'Travel literature', 'Experimental literature', 'Pornography', 'Postmodernism', 'Industrial novel', 'Time travel', 'Prose poetry', 'Farce', 'Crime Fiction', 'Catastrophic literature', 'Utopian fiction', 'Metaphysics', \"Boys' school stories\", 'Alternate history', 'Comics', 'Picture book', 'Existentialism', 'Transhumanism', 'Non-fiction', 'Ghost story', 'Morality play', 'Adventure', 'Urban fiction', 'Space opera', 'Picaresque novel', 'Romantic comedy', 'Reference', 'Future history', 'Comic fantasy', 'Popular science', 'Techno-thriller', 'Georgian romance', 'Conspiracy fiction', 'Fantastique', 'Social novel', 'Satire', 'Social science fiction', 'Novella', 'Collage', 'Military history', 'Science Fiction', 'Detective fiction', 'Albino bias', 'Romance novel', 'Magic realism', 'Serial', 'Whodunit', 'Supernatural', 'Fiction', 'Biographical novel', 'Subterranean fiction', 'Low fantasy', 'Spy fiction', 'Gothic fiction', 'Mathematics', 'Sports', 'Pastiche', 'Psychology', 'True crime', 'Colonial United States romance', 'Speculative fiction', 'Politics', 'New Weird', 'Transgender and transsexual fiction', 'Personal journal', 'Zombies in popular culture', 'Dying Earth subgenre', 'Absurdist fiction', 'Marketing', 'Travel', 'Biography', 'Fantasy', 'Indian chick lit', 'Anthology', 'English public-school stories', 'Locked room mystery', 'Literary criticism', \"Children's literature\", 'Business', 'Popular culture', 'Dystopia', 'Social commentary', 'Suspense', 'Sword and sorcery', 'Neuroscience', 'Juvenile fantasy', 'Graphic novel', 'Vampire fiction', 'Foreign legion', 'Fable', 'Novel', 'Fairy tale', 'LGBT literature', 'Space western', 'Gamebook', 'Urban fantasy', 'Literary fiction', 'Sea story', 'Superhero fiction', 'Naval adventure', 'Literary theory', 'Autobiographical comics', 'Social criticism', 'Adventure novel', 'Autobiography', 'Science fantasy', 'Regency romance', 'Gay Themed', 'Edisonade', 'Religious text', 'Human extinction', 'Modernism', 'Postcyberpunk', 'Epic Science Fiction and Fantasy', 'Bangsian fantasy', 'Play', 'Lost World', 'Sword and planet', 'Chick lit', 'Light novel', 'Encyclopedia', 'Poetry', 'Elizabethan romance', 'Parody', 'Contemporary fantasy', 'Alien invasion', 'Planetary romance', 'Biopunk', 'Autobiographical novel', 'Humour', 'Hard science fiction', 'Horror', 'Historical whodunnit', 'Religion', 'Bildungsroman', 'Self-help', 'Comedy of manners', 'American Gothic Fiction', 'Soft science fiction', 'Fantasy of manners', 'Fairytale fantasy', 'Fictional crossover', 'Western fiction', 'Western', 'Anti-war', 'Memoir', 'Roman à clef', 'Mashup', 'Non-fiction novel', 'Utopian and dystopian fiction', 'Künstlerroman', 'Wuxia', 'Economics', 'Steampunk', 'Cookbook', 'Gay novel', 'Historical fantasy', 'Bit Lit', 'Cabal', 'Young adult literature', 'Cozy'}\n"
     ]
    }
   ],
   "source": [
    "genres = set()\n",
    "for x in book_summs['formatted_genre']:\n",
    "    genres.update(x)\n",
    "print('# of unique genres:', len(genres))\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ed7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab, reverse_vocab = generate_vocab_map(book_summs)\n",
    "genre_map = generate_genre_map(list(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ee4a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tokenized_title  \\\n",
      "7456   [strands, starlight]   \n",
      "7366        [pentagon, spy]   \n",
      "4164       [beyond, apollo]   \n",
      "12912       [killing, star]   \n",
      "1936       [virtual, light]   \n",
      "\n",
      "                                       tokenized_summary  \\\n",
      "7456   [miriam, 's, parents, expel, young, age, power...   \n",
      "7366   [valuable, antique, weathervanes, stolen, penn...   \n",
      "4164   [novel, 's, protagonist, harry, m., evans, ,, ...   \n",
      "12912  [late, 21st, century, seems, like, good, time,...   \n",
      "1936   [plot, centers, around, chevette, washington, ...   \n",
      "\n",
      "                                         formatted_genre  \n",
      "7456                      [Speculative fiction, Fantasy]  \n",
      "7366                        [Mystery, Detective fiction]  \n",
      "4164              [Science Fiction, Speculative fiction]  \n",
      "12912                    [Thriller, Speculative fiction]  \n",
      "1936   [Cyberpunk, Science Fiction, Utopian and dysto...  \n",
      "            tokenized_title  \\\n",
      "7456   [strands, starlight]   \n",
      "7366        [pentagon, spy]   \n",
      "4164       [beyond, apollo]   \n",
      "12912       [killing, star]   \n",
      "1936       [virtual, light]   \n",
      "\n",
      "                                       tokenized_summary formatted_genre  \n",
      "7456   [miriam, 's, parents, expel, young, age, power...      [126, 137]  \n",
      "7366   [valuable, antique, weathervanes, stolen, penn...       [24, 107]  \n",
      "4164   [novel, 's, protagonist, harry, m., evans, ,, ...      [106, 126]  \n",
      "12912  [late, 21st, century, seems, like, good, time,...       [56, 126]  \n",
      "1936   [plot, centers, around, chevette, washington, ...  [52, 106, 215]  \n"
     ]
    }
   ],
   "source": [
    "# We only care about tokenized and formatted data from the original dataset\n",
    "book_summs = book_summs.sample(frac=1)\n",
    "X = book_summs[['tokenized_title', 'tokenized_summary', 'formatted_genre']].copy()\n",
    "print(X.head())\n",
    "mapped_genres = []\n",
    "for x in X['formatted_genre']:\n",
    "    m = []\n",
    "    for g in x:\n",
    "        m.append(genre_map[g])\n",
    "    mapped_genres.append(m)\n",
    "X['formatted_genre'] = mapped_genres\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4a9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data\n",
    "from SummaryDataset import split_train_val_test\n",
    "X_train, X_val, X_test = split_train_val_test(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3150bb",
   "metadata": {},
   "source": [
    "## TODO: Review midterm PP1 for pytorch implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3949cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SummaryDataset import SummaryDataset\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "train_dataset = SummaryDataset(train_vocab, X_train)\n",
    "val_dataset = SummaryDataset(train_vocab, X_val)\n",
    "test_dataset = SummaryDataset(train_vocab, X_test)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler = RandomSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c79354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from SummaryDataset import collate_fn\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077d381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GenreClassifier import ClassificationModel\n",
    "\n",
    "model = None\n",
    "model = ClassificationModel(vocab_size=len(train_vocab.keys()), embedding_dim=300, hidden_dim=1, output_dim=len(genres), num_layers=1, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f4eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "criterion, optimizer = None, None\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b089e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# returns the total loss calculated from criterion\n",
    "def train_loop(model, criterion, iterator):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        y_pred = model(x).round()\n",
    "        loss = criterion(torch.flatten(y_pred),torch.tensor(y, dtype=torch.float))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    return total_loss\n",
    "\n",
    "# returns:\n",
    "# - true: a Python boolean array of all the ground truth values\n",
    "#         taken from the dataset iterator\n",
    "# - pred: a Python boolean array of all model predictions.\n",
    "def val_loop(model, criterion, iterator):\n",
    "    true, pred = [], []\n",
    "    for x, y in tqdm(iterator):\n",
    "        for t in y:\n",
    "            true.append(t)\n",
    "        y_pred = model(x).round()\n",
    "        for p in y_pred:\n",
    "            pred.append(p)\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c202152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/81 [00:00<?, ?it/s]/Users/cindyzastudil/Documents/cis5590-project/SummaryDataset.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.long)\n",
      "  0%|                                                    | 0/81 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 227)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#from src.eval_utils import binary_macro_f1, accuracy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m true, pred \u001b[38;5;241m=\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(binary_macro_f1(true, pred)) # TODO: CHANGE METRIC CALCULATIONS\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(accuracy(true, pred)) # TODO: CHANGE METRIC CALCULATIONS\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m, in \u001b[0;36mval_loop\u001b[0;34m(model, criterion, iterator)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[1;32m     23\u001b[0m     true\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[0;32m---> 24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m y_pred:\n\u001b[1;32m     26\u001b[0m     pred\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/Documents/cis5590-project/GenreClassifier.py:58\u001b[0m, in \u001b[0;36mClassificationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m x, (h0, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(torch\u001b[38;5;241m.\u001b[39mcat((h0[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,:,:], h0[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:]), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/activation.py:1198\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:1512\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1510\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1512\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1514\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 227)"
     ]
    }
   ],
   "source": [
    "#from src.eval_utils import binary_macro_f1, accuracy\n",
    "true, pred = val_loop(model, criterion, val_iterator)\n",
    "#print(binary_macro_f1(true, pred)) # TODO: CHANGE METRIC CALCULATIONS\n",
    "#print(accuracy(true, pred)) # TODO: CHANGE METRIC CALCULATIONS\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eda085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
